from typing import List, Union, Optional, Tuple
from enum import IntEnum

# from qlib.data.dataset.loader import QlibDataLoader
from qlib.data import D
import qlib

qlib.init()

import abc
import numpy as np
import pandas as pd
import torch
import os
import warnings
import random
import math


class FeatureType(IntEnum):
    OPEN = 0
    HIGH = 1
    LOW = 2
    CLOSE = 3
    PRE_CLOSE = 4
    CHANGE = 5
    PCT_CHG = 6
    VOLUME = 7
    AMOUNT = 8
    ADJ_FACTOR = 9


# Mapping from Qlib expression to (filename, column_name_in_file)
EXPR_TO_FILE_COL_MAP = {
    # stock.csv columns
    "$open": ("stock.csv", "open"),
    "$high": ("stock.csv", "high"),
    "$low": ("stock.csv", "low"),
    "$close": ("stock.csv", "close"),
    "$pre_close": ("stock.csv", "pre_close"),
    "$change": ("stock.csv", "change"),
    "$pct_chg": ("stock.csv", "pct_chg"),
    "$volume": ("stock.csv", "vol"),  # User request: volume -> vol
    "$amount": ("stock.csv", "amount"),
    # Separate file for adj_factor
    "$adj_factor": (
        "factor.csv",
        "adj_factor",
    ),  # User request: adj_factor is in adj_factor.csv in the same directory level
}


# Add dataloader from qlib
class DataLoader(abc.ABC):
    """
    DataLoader is designed for loading raw data from original data source.
    """

    @abc.abstractmethod
    def load(self, instruments, start_time=None, end_time=None) -> pd.DataFrame:
        """
        load the data as pd.DataFrame.

        Example of the data (The multi-index of the columns is optional.):

            .. code-block:: text

                                        feature                                                             label
                                        $close     $volume     Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0
                ts_code    trade_date
                300135.SZ  20140508    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032


        Parameters
        ----------
        instruments : str or dict
            it can either be the market name or the config file of instruments generated by InstrumentProvider.
            If the value of instruments is None, it means that no filtering is done.
        start_time : str
            start of the time range.
        end_time : str
            end of the time range.

        Returns
        -------
        pd.DataFrame:
            data load from the under layer source

        Raise
        -----
        KeyError:
            if the instruments filter is not supported, raise KeyError
        """


class DLWParser(DataLoader):
    """
    (D)ata(L)oader (W)ith (P)arser for features and names

    Extracting this class so that QlibDataLoader and other dataloaders(such as QdbDataLoader) can share the fields.
    """

    def __init__(self, config: Union[list, tuple, dict]):
        """
        Parameters
        ----------
        config : Union[list, tuple, dict]
            Config will be used to describe the fields and column names

            .. code-block::

                <config> := {
                    "group_name1": <fields_info1>
                    "group_name2": <fields_info2>
                }
                or
                <config> := <fields_info>

                <fields_info> := ["expr", ...] | (["expr", ...], ["col_name", ...])
                # NOTE: list or tuple will be treated as the things when parsing
        """
        # config_example = {
        #     "feature": ["$close", "$volume"],  # Feature group: load close price and volume, column names are same as expressions
        #     "label": (["Ref($close, 1)"], ["next_day_close"]) # Label group: load next day's close price and name it "next_day_close"
        # }
        self.is_group = isinstance(config, dict)

        if self.is_group:
            # If config is a dictionary, load feature group and label group separately
            self.fields = {
                grp: self._parse_fields_info(fields_info)
                for grp, fields_info in config.items()
            }
        else:
            self.fields = self._parse_fields_info(config)

    def _parse_fields_info(self, fields_info: Union[list, tuple]) -> Tuple[list, list]:
        if len(fields_info) == 0:
            raise ValueError("The size of fields must be greater than 0")

        if not isinstance(fields_info, (list, tuple)):
            raise TypeError("Unsupported type")

        if isinstance(fields_info[0], str):
            exprs = names = fields_info
        elif isinstance(fields_info[0], (list, tuple)):
            exprs, names = fields_info
        else:
            raise NotImplementedError(f"This type of input is not supported")
        return exprs, names

    @abc.abstractmethod
    def load_group_df(
        self,
        instruments,
        exprs: list,
        names: list,
        start_time: Union[str, pd.Timestamp] = None,
        end_time: Union[str, pd.Timestamp] = None,
        gp_name: str = None,
    ) -> pd.DataFrame:
        """
        load the dataframe for specific group

        Parameters
        ----------
        instruments :
            the instruments.
        exprs : list
            the expressions to describe the content of the data.
        names : list
            the name of the data.

        Returns
        -------
        pd.DataFrame:
            the queried dataframe.
        """

    def load(self, instruments=None, start_time=None, end_time=None) -> pd.DataFrame:
        if self.is_group:
            df = pd.concat(
                {
                    grp: self.load_group_df(
                        instruments, exprs, names, start_time, end_time, grp
                    )
                    for grp, (exprs, names) in self.fields.items()
                },
                axis=1,
            )
        else:
            exprs, names = self.fields
            df = self.load_group_df(instruments, exprs, names, start_time, end_time)
        return df


class Final_DataLoader(DLWParser):
    """Same as QlibDataLoader. The fields can be define by config"""

    def __init__(
        self,
        config: Tuple[list, tuple, dict],
        filter_pipe: List = None,
        swap_level: bool = True,
        freq: Union[str, dict] = "day",
        inst_processors: Union[dict, list] = None,
    ):
        """
        Parameters
        ----------
        config : Tuple[list, tuple, dict]
            Please refer to the doc of DLWParser
        filter_pipe :
            Filter pipe for the instruments
        swap_level :
            Whether to swap level of MultiIndex
        freq:  dict or str
            If type(config) == dict and type(freq) == str, load config data using freq.
            If type(config) == dict and type(freq) == dict, load config[<group_name>] data using freq[<group_name>]
        inst_processors: dict | list
            If inst_processors is not None and type(config) == dict; load config[<group_name>] data using inst_processors[<group_name>]
            If inst_processors is a list, then it will be applied to all groups.
        """
        self.filter_pipe = filter_pipe
        self.swap_level = swap_level
        self.freq = freq

        # sample
        self.inst_processors = inst_processors if inst_processors is not None else {}
        assert isinstance(
            self.inst_processors, (dict, list)
        ), f"inst_processors(={self.inst_processors}) must be dict or list"

        super().__init__(config)

        if self.is_group:
            # check sample config
            if isinstance(freq, dict):
                for _gp in config.keys():
                    if _gp not in freq:
                        raise ValueError(f"freq(={freq}) missing group(={_gp})")
                assert (
                    self.inst_processors
                ), f"freq(={self.freq}), inst_processors(={self.inst_processors}) cannot be None/empty"

    def load_group_df(
        self,
        instruments,
        exprs: list,
        names: list,
        start_time: Union[str, pd.Timestamp] = None,
        end_time: Union[str, pd.Timestamp] = None,
        gp_name: str = None,
    ) -> pd.DataFrame:
        if instruments is None:
            warnings.warn("`instruments` is not set, will load all stocks")
            instruments = "all"
        if isinstance(instruments, str):
            instruments = D.instruments(instruments, filter_pipe=self.filter_pipe)
        elif self.filter_pipe is not None:
            warnings.warn(
                "`filter_pipe` is not None, but it will not be used with `instruments` as list"
            )

        freq = self.freq[gp_name] if isinstance(self.freq, dict) else self.freq
        inst_processors = (
            self.inst_processors
            if isinstance(self.inst_processors, list)
            else self.inst_processors.get(gp_name, [])
        )
        df: pd.DataFrame = D.features(
            instruments,
            exprs,
            start_time,
            end_time,
            freq=freq,
            inst_processors=inst_processors,
        )
        df.columns = names
        if self.swap_level:
            df = (
                df.swaplevel().sort_index()
            )  # NOTE: if swaplevel, return <datetime, instrument>
        return df


class StockData:
    def __init__(
        self,
        start_time: str,
        end_time: str,
        # todo backtrack_days: 100
        # todo future_days: 30
        max_backtrack_days: int = 100,
        max_future_days: int = 30,
        features: Optional[List[FeatureType]] = None,
        preloaded_data: Optional[Tuple[torch.Tensor, pd.Index, pd.Index]] = None,
        device: str = "cpu",
    ) -> None:
        self.max_backtrack_days = max_backtrack_days
        self.max_future_days = max_future_days
        self._start_time = start_time
        self._end_time = end_time
        self._features = features if features is not None else list(FeatureType)
        self.device = device
        data_tup = preloaded_data if preloaded_data is not None else self._get_data()
        self.data, self._dates, self._stock_ids = data_tup

    def _load_exprs(self, exprs: Union[str, List[str]]) -> pd.DataFrame:
        # This evaluates an expression on the data and returns the dataframe
        # It might throw on illegal expressions like "Ref(constant, dtime)"

        # Convert exprs to a list
        if not isinstance(exprs, list):
            exprs = [exprs]

        # The calendar's properties can be retained
        cal: np.ndarray = D.calendar()

        start_index = cal.searchsorted(pd.Timestamp(self._start_time))  # type: ignore
        end_index = cal.searchsorted(pd.Timestamp(self._end_time))  # type: ignore
        real_start_time = cal[start_index - self.max_backtrack_days]

        # start_index, end_index and real_start_time are all np.ndarray
        if cal[end_index] != pd.Timestamp(self._end_time):
            end_index -= 1
        real_end_time = cal[end_index + self.max_future_days]

        # !attention, it is the random
        return self._get_pd_data_random(exprs, real_start_time, real_end_time)

    

    def _load_original_dataset(self, path_or_obj, index_col=[0, 1]):
        """load dataset from multiple file formats"""
        # Mainly for .csv file, also supports other formats
        if isinstance(path_or_obj, pd.DataFrame):
            return path_or_obj
        if not os.path.exists(path_or_obj):
            raise ValueError(f"file {path_or_obj} doesn't exist")
        _, extension = os.path.splitext(path_or_obj)
        if extension == ".h5":
            return pd.read_hdf(path_or_obj)
        elif extension == ".pkl":
            return pd.read_pickle(path_or_obj)
        elif extension == ".csv":
            # Explicitly specify date format as YYYYMMDD to resolve UserWarning
            return pd.read_csv(
                path_or_obj, parse_dates=True, date_format="%Y%m%d", index_col=index_col
            )
        raise ValueError(f"unsupported file type `{extension}`")

    def _get_pd_data(
        self, exprs: Union[str, List[str]], real_start_time, real_end_time
    ) -> pd.DataFrame:
        all_daily_dfs = []

        cal: np.ndarray = D.calendar()

        start_cal_idx = cal.searchsorted(pd.Timestamp(real_start_time))
        end_cal_idx = cal.searchsorted(pd.Timestamp(real_end_time), side="right")

        print(
            f"DEBUG: Attempting to load data from {real_start_time.strftime('%Y-%m-%d')} to {real_end_time.strftime('%Y-%m-%d')}. "
        )
        print(f"DEBUG: Calendar index range: {start_cal_idx} to {end_cal_idx}")

        for i, day_ts in enumerate(cal[start_cal_idx:end_cal_idx]):
            date_str = day_ts.strftime("%Y%m%d")
            base_dir = os.path.join("./data/tushare", date_str)

            # print(f"DEBUG: Processing date: {date_str} (Day {i+1} in calendar)")

            daily_df_parts = (
                {}
            )  # Stores data columns loaded from different files for the current day

            for expr in exprs:
                if expr not in EXPR_TO_FILE_COL_MAP:
                    warnings.warn(
                        f"Unsupported expression: {expr}. Skipping this expression."
                    )
                    continue

                file_suffix, col_name_in_file = EXPR_TO_FILE_COL_MAP[expr]
                file_path = os.path.join(base_dir, file_suffix)

                # print(
                #     f"DEBUG: Checking file: {file_path} (corresponding to expression: {expr})"
                # )

                if os.path.exists(file_path):
                    try:
                        # Load data for the specified file, with (ts_code, trade_date) as index
                        temp_df = self._load_original_dataset(
                            file_path, index_col=["ts_code", "trade_date"]
                        )
                        if col_name_in_file in temp_df.columns:
                            # Use original expression as column name for the final DataFrame
                            daily_df_parts[expr] = temp_df[col_name_in_file]
                        else:
                            warnings.warn(
                                f"Missing column '{col_name_in_file}' in file {file_path} (corresponding to expression {expr}) for date {date_str}. Skipping this column."
                            )
                    except Exception as e:
                        warnings.warn(
                            f"Failed to load file {file_path}: {e}. Skipping data from this file."
                        )
                else:
                    warnings.warn(
                        f"Data file not found for date {date_str}: {file_path} (corresponding to expression {expr}). Skipping data from this file."
                    )

            if daily_df_parts:
                # Concatenate all loaded data columns for the current day into a single DataFrame
                # keys are column names (i.e., Qlib expressions), values are Series
                current_day_df = pd.DataFrame.from_dict(
                    daily_df_parts, orient="columns"
                )

                # Ensure index names are correctly set, which is important for subsequent concat and swaplevel
                current_day_df.index.names = ["ts_code", "trade_date"]

                # No longer filtering by self._instrument, directly using all market stocks

                all_daily_dfs.append(current_day_df)
                # DEBUG: Data loaded successfully for date {date_str}. Includes expressions: {list(daily_df_parts.keys())}. Shape: {current_day_df.shape}
                # DEBUG: Date {date_str} failed to load any valid data.
            else:
                pass  # Already handled by the warnings above

        if not all_daily_dfs:
            raise ValueError(
                f"No data found for the specified period: from {real_start_time.strftime('%Y-%m-%d')} to {real_end_time.strftime('%Y-%m-%d')}"
            )

        combined_df = pd.concat(all_daily_dfs)

        # _load_original_dataset returns a MultiIndex of (ts_code, trade_date).
        # To match the (trade_date, ts_code) format expected by qlib, we need to swap the index levels.
        if (
            combined_df.index.nlevels == 2
            and combined_df.index.names[0] == "ts_code"
            and combined_df.index.names[1] == "trade_date"
        ):
            combined_df = combined_df.swaplevel(0, 1).sort_index()

        print(f"DEBUG: Final combined DataFrame shape: {combined_df.shape}")
        # print(combined_df)
        return combined_df

    def _get_pd_data_random(
        self, exprs: Union[str, List[str]], real_start_time, real_end_time
    ) -> pd.DataFrame:
        all_daily_dfs = []
        cal: np.ndarray = D.calendar()
        start_cal_idx = cal.searchsorted(pd.Timestamp(real_start_time))
        end_cal_idx = cal.searchsorted(pd.Timestamp(real_end_time), side="right")

        for i, day_ts in enumerate(cal[start_cal_idx:end_cal_idx]):
            date_str = day_ts.strftime("%Y%m%d")
            base_dir = os.path.join("./data/tushare", date_str)
            daily_df_parts = {}

            for expr in exprs:
                if expr not in EXPR_TO_FILE_COL_MAP:
                    warnings.warn(
                        f"Unsupported expression: {expr}. Skipping this expression."
                    )
                    continue

                file_suffix, col_name_in_file = EXPR_TO_FILE_COL_MAP[expr]
                file_path = os.path.join(base_dir, file_suffix)

                if os.path.exists(file_path):
                    try:
                        temp_df = self._load_original_dataset(
                            file_path, index_col=["ts_code", "trade_date"]
                        )
                        if col_name_in_file in temp_df.columns:
                            daily_df_parts[expr] = temp_df[col_name_in_file]
                        else:
                            warnings.warn(
                                f"Missing column '{col_name_in_file}' in file {file_path} (corresponding to expression {expr}) for date {date_str}. Skipping this column."
                            )
                    except Exception as e:
                        warnings.warn(
                            f"Failed to load file {file_path}: {e}. Skipping data from this file."
                        )
                else:
                    warnings.warn(
                        f"Data file not found for date {date_str}: {file_path} (corresponding to expression {expr}). Skipping data from this file."
                    )

            if daily_df_parts:
                current_day_df = pd.DataFrame.from_dict(
                    daily_df_parts, orient="columns"
                )
                current_day_df.index.names = ["ts_code", "trade_date"]

                # 随机选取1/10的股票
                ts_codes = current_day_df.index.get_level_values("ts_code").unique()
                n_select = max(1, math.ceil(len(ts_codes) / 10))
                selected_ts_codes = random.sample(list(ts_codes), n_select)
                current_day_df = current_day_df.loc[(selected_ts_codes, slice(None)), :]

                all_daily_dfs.append(current_day_df)

        if not all_daily_dfs:
            raise ValueError(
                f"No data found for the specified period: from {real_start_time.strftime('%Y-%m-%d')} to {real_end_time.strftime('%Y-%m-%d')}"
            )

        combined_df = pd.concat(all_daily_dfs)
        if (
            combined_df.index.nlevels == 2
            and combined_df.index.names[0] == "ts_code"
            and combined_df.index.names[1] == "trade_date"
        ):
            combined_df = combined_df.swaplevel(0, 1).sort_index()
        return combined_df

    def _get_data(self) -> Tuple[torch.Tensor, pd.Index, pd.Index]:
        features = ["$" + f.name.lower() for f in self._features]
        df = self._load_exprs(features)
        df = df.stack().unstack(level=1)
        dates = df.index.levels[0]  # type: ignore
        stock_ids = df.columns
        values = df.values
        values = values.reshape((-1, len(features), values.shape[-1]))  # type: ignore
        return (
            torch.tensor(values, dtype=torch.float, device=self.device),
            dates,
            stock_ids,
        )

    def __getitem__(self, slc: slice) -> "StockData":
        "Get a subview of the data given a date slice or an index slice."
        if slc.step is not None:
            raise ValueError("Only support slice with step=None")
        if isinstance(slc.start, str):
            return self[self.find_date_slice(slc.start, slc.stop)]
        start, stop = slc.start, slc.stop
        start = start if start is not None else 0
        stop = (
            (stop if stop is not None else self.n_days)
            + self.max_future_days
            + self.max_backtrack_days
        )
        start = max(0, start)
        stop = min(self.data.shape[0], stop)
        idx_range = slice(start, stop)
        data = self.data[idx_range]
        remaining = (
            data.isnan()
            .reshape(-1, data.shape[-1])
            .all(dim=0)
            .logical_not()
            .nonzero()
            .flatten()
        )
        data = data[:, :, remaining]
        return StockData(
            start_time=self._dates[start + self.max_backtrack_days].strftime(
                "%Y-%m-%d"
            ),
            end_time=self._dates[stop - 1 + self.max_future_days].strftime("%Y-%m-%d"),
            max_backtrack_days=self.max_backtrack_days,
            max_future_days=self.max_future_days,
            features=self._features,
            device=self.device,
            preloaded_data=(
                data,
                self._dates[idx_range],
                self._stock_ids[remaining.tolist()],
            ),
        )

    def find_date_index(self, date: str, exclusive: bool = False) -> int:
        ts = pd.Timestamp(date)
        idx: int = self._dates.searchsorted(ts)  # type: ignore
        if exclusive and self._dates[idx] == ts:
            idx += 1
        idx -= self.max_backtrack_days
        if idx < 0 or idx > self.n_days:
            raise ValueError(
                f"Date {date} is out of range: available [{self._start_time}, {self._end_time}]"
            )
        return idx

    def find_date_slice(
        self, start_time: Optional[str] = None, end_time: Optional[str] = None
    ) -> slice:
        """
        Find a slice of indices corresponding to the given date range.
        For the input, both ends are inclusive. The output is a normal left-closed right-open slice.
        """
        start = None if start_time is None else self.find_date_index(start_time)
        stop = (
            None
            if end_time is None
            else self.find_date_index(end_time, exclusive=False)
        )
        return slice(start, stop)

    @property
    def n_features(self) -> int:
        return len(self._features)

    @property
    def n_stocks(self) -> int:
        # Get the number of stocks from self._stock_ids
        return len(self._stock_ids)

    @property
    def n_days(self) -> int:
        return self.data.shape[0] - self.max_backtrack_days - self.max_future_days

    @property
    def stock_ids(self) -> pd.Index:
        return self._stock_ids

    def make_dataframe(
        self,
        data: Union[torch.Tensor, List[torch.Tensor]],
        columns: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        """
        Parameters:
        - `data`: a tensor of size `(n_days, n_stocks[, n_columns])`, or
        a list of tensors of size `(n_days, n_stocks)`
        - `columns`: an optional list of column names
        """
        if isinstance(data, list):
            data = torch.stack(data, dim=2)
        if len(data.shape) == 2:
            data = data.unsqueeze(2)
        if columns is None:
            columns = [str(i) for i in range(data.shape[2])]
        n_days, n_stocks, n_columns = data.shape
        if self.n_days != n_days:
            raise ValueError(
                f"number of days in the provided tensor ({n_days}) doesn't "
                f"match that of the current StockData ({self.n_days})"
            )
        if self.n_stocks != n_stocks:
            raise ValueError(
                f"number of stocks in the provided tensor ({n_stocks}) doesn't "
                f"match that of the current StockData ({self.n_stocks})"
            )
        if len(columns) != n_columns:
            raise ValueError(
                f"size of columns ({len(columns)}) doesn't match with "
                f"tensor feature count ({data.shape[2]})"
            )
        if self.max_future_days == 0:
            date_index = self._dates[self.max_backtrack_days :]
        else:
            date_index = self._dates[self.max_backtrack_days : -self.max_future_days]
        index = pd.MultiIndex.from_product([date_index, self._stock_ids])
        data = data.reshape(-1, n_columns)
        return pd.DataFrame(data.detach().cpu().numpy(), index=index, columns=columns)



